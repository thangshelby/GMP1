{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN4Ylzj8sUg1",
        "outputId": "4d205a13-e882-46c0-c7eb-8d04a0324379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m609.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting torch\n",
            "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting filelock (from transformers)\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Collecting fsspec>=0.8.5 (from torch)\n",
            "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.0 (from torch)\n",
            "  Downloading triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m590.7/899.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:28\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/bin/pip\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    with self.main_context():\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/cli/command_context.py\", line 19, in main_context\n",
            "    with self._main_context:\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 610, in __exit__\n",
            "    raise exc_details[1]\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 595, in __exit__\n",
            "    if cb(*exc_details):\n",
            "       ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/utils/temp_dir.py\", line 42, in global_tempdir_manager\n",
            "    with ExitStack() as stack:\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 610, in __exit__\n",
            "    raise exc_details[1]\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 595, in __exit__\n",
            "    if cb(*exc_details):\n",
            "       ^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/utils/temp_dir.py\", line 169, in __exit__\n",
            "    self.cleanup()\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/utils/temp_dir.py\", line 212, in cleanup\n",
            "    rmtree(self._path, ignore_errors=False)\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_vendor/tenacity/__init__.py\", line 291, in wrapped_f\n",
            "    return self(f, *args, **kw)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_vendor/tenacity/__init__.py\", line 381, in __call__\n",
            "    do = self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_vendor/tenacity/__init__.py\", line 316, in iter\n",
            "    return fut.result()\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_vendor/tenacity/__init__.py\", line 384, in __call__\n",
            "    result = fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/ngothang/github/GMP1/server/venv/lib/python3.12/site-packages/pip/_internal/utils/misc.py\", line 144, in rmtree\n",
            "    shutil.rmtree(dir, onexc=handler)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 785, in rmtree\n",
            "    _rmtree_safe_fd(fd, path, onexc)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 715, in _rmtree_safe_fd\n",
            "    os.unlink(entry.name, dir_fd=topfd)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "# Chạy khi chưa cài và chạy 1 lần thôi\n",
        "# Chạy ô này để cài đặt thư viện\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CQsHi6YZsUqP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import pipeline  # Thư viện NLP chính\n",
        "import os\n",
        "\n",
        "# Thư viện cho Giai đoạn 2 (Machine Learning)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Cài đặt để hiển thị đầy đủ các cột khi in\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZJ1I3NpsUsu",
        "outputId": "e18ad3e7-03c6-4a40-defe-f2d937fbfc37"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# # --- QUAN TRỌNG: HÃY SỬA ĐƯỜNG DẪN NÀY ---\n",
        "# # Hãy đảm bảo đường dẫn kết thúc bằng dấu '/'\n",
        "# BASE_DRIVE_PATH = '/content/drive/MyDrive/big_data/data/'\n",
        "# print(f\"Đã kết nối Drive. Thư mục làm việc: {BASE_DRIVE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_2021= pd.read_csv('data/2021_merged.csv')\n",
        "file_2022_2023 = pd.read_csv('data/2022-2023_merged.csv')\n",
        "file_2024_2025 = pd.read_csv('data/2024-2025_merged.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "h_bwzVKdsUvG",
        "outputId": "c8c43121-d590-4ac5-8b1f-73c1c06cd52c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đã đọc thành công 3 file CSV từ Google Drive.\n",
            "Dữ liệu gốc (2021): 9986 dòng\n",
            "Dữ liệu (2022-2023): 63446 dòng\n",
            "Dữ liệu (2024-2025): 57515 dòng\n",
            "==> TỔNG DỮ LIỆU ĐÃ GỘP: 130947 dòng\n",
            "\n",
            "Thông tin dữ liệu (Data Info) sau khi gộp:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 130947 entries, 0 to 130946\n",
            "Data columns (total 17 columns):\n",
            " #   Column       Non-Null Count   Dtype         \n",
            "---  ------       --------------   -----         \n",
            " 0   datepost     130947 non-null  object        \n",
            " 1   title        130947 non-null  object        \n",
            " 2   Ticker       130947 non-null  object        \n",
            " 3   Market       130947 non-null  object        \n",
            " 4   Date         130947 non-null  datetime64[ns]\n",
            " 5   Open         130947 non-null  float64       \n",
            " 6   High         130947 non-null  float64       \n",
            " 7   Low          130947 non-null  float64       \n",
            " 8   Close        130947 non-null  float64       \n",
            " 9   Volume       130947 non-null  int64         \n",
            " 10  lead1_date   130882 non-null  object        \n",
            " 11  lead1_close  130882 non-null  float64       \n",
            " 12  prev5_date   130867 non-null  object        \n",
            " 13  prev5_close  130867 non-null  float64       \n",
            " 14  pct_inday    130947 non-null  object        \n",
            " 15  pct_lead1    130947 non-null  object        \n",
            " 16  pct_prev5    130947 non-null  object        \n",
            "dtypes: datetime64[ns](1), float64(6), int64(1), object(9)\n",
            "memory usage: 17.0+ MB\n",
            "\n",
            "5 dòng dữ liệu ĐẦU TIÊN (cũ nhất):\n",
            "   datepost                                              title Ticker Market  \\\n",
            "0  6/1/2021  IMP: Danh sách cổ đông nhà nước, cổ đông chiến...    IMP    HSX   \n",
            "1  6/1/2021  Sacombank 'Khai hè rực rỡ - Mở màn triệu deal'...    STB    HSX   \n",
            "2  6/1/2021  MSB: Thông báo về việc chuyển địa chỉ của phòn...    MSB    HSX   \n",
            "3  6/1/2021  MSB: Chuyển địa chỉ của phòng Giao dịch Minh Khai    MSB    HSX   \n",
            "4  6/1/2021  FPT lên tiếng về tiến độ xử lý tình trạng quá ...    FPT    HSX   \n",
            "\n",
            "        Date     Open     High      Low    Close    Volume lead1_date  \\\n",
            "0 2021-06-01  29.0290  29.1091  29.0290  29.1123     30300   6/2/2021   \n",
            "1 2021-06-01  33.5000  33.5500  32.3500  32.4000  47063500   6/2/2021   \n",
            "2 2021-06-01  12.6063  12.9021  12.2877  12.7431  13815600   6/2/2021   \n",
            "3 2021-06-01  12.6063  12.9021  12.2877  12.7431  13815600   6/2/2021   \n",
            "4 2021-06-01  44.6005  44.9049  44.1438  44.1959   3495000   6/2/2021   \n",
            "\n",
            "   lead1_close prev5_date  prev5_close pct_inday pct_lead1 pct_prev5  \n",
            "0      29.3526  5/26/2021      29.0723     0.29%     0.82%    -0.14%  \n",
            "1      32.2000  5/26/2021      29.5000    -3.34%    -0.62%    -9.38%  \n",
            "2      12.6066  5/26/2021      11.4916     1.08%    -1.08%   -10.34%  \n",
            "3      12.6066  5/26/2021      11.4916     1.08%    -1.08%   -10.34%  \n",
            "4      43.3840  5/26/2021      41.6206    -0.91%    -1.85%    -6.00%  \n"
          ]
        }
      ],
      "source": [
        "# 1. Đảm bảo 3 file CSV của bạn nằm trong thư mục:\n",
        "\n",
        "# --- Sửa lại tên file nếu cần ---\n",
        "file_2021= pd.read_csv('data/2021_merged.csv')\n",
        "file_2022_2023 = pd.read_csv('data/2022-2023_merged.csv')\n",
        "file_2024_2025 = pd.read_csv('data/2024-2025_merged.csv')\n",
        "\n",
        "\n",
        "try:\n",
        "    # 2. Đọc từng file\n",
        "    df_2021 = file_2021\n",
        "    df_2022_2023 = file_2022_2023\n",
        "    df_2024_2025 = file_2024_2025\n",
        "\n",
        "    print(\"Đã đọc thành công 3 file CSV từ Google Drive.\")\n",
        "\n",
        "    # 3. Gộp 3 DataFrame\n",
        "    df_list = [df_2021, df_2022_2023, df_2024_2025]\n",
        "    df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "    print(f\"Dữ liệu gốc (2021): {len(df_2021)} dòng\")\n",
        "    print(f\"Dữ liệu (2022-2023): {len(df_2022_2023)} dòng\")\n",
        "    print(f\"Dữ liệu (2024-2025): {len(df_2024_2025)} dòng\")\n",
        "    print(f\"==> TỔNG DỮ LIỆU ĐÃ GỘP: {len(df)} dòng\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"LỖI: Không tìm thấy file. {e}\")\n",
        "    print(\"Vui lòng kiểm tra lại ĐƯỜNG DẪN và TÊN FILE trong Drive.\")\n",
        "    raise e\n",
        "\n",
        "# 4. Chuyển đổi cột 'Date' sang datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')\n",
        "\n",
        "# 5. Sắp xếp lại dữ liệu theo thứ tự thời gian (cũ đến mới)\n",
        "df = df.sort_values(by='Date', ascending=True).reset_index(drop=True)\n",
        "\n",
        "# 6. Kiểm tra dữ liệu\n",
        "print(\"\\nThông tin dữ liệu (Data Info) sau khi gộp:\")\n",
        "df.info()\n",
        "print(\"\\n5 dòng dữ liệu ĐẦU TIÊN (cũ nhất):\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZM20z5tcsUxh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang tải mô hình NLP...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1783\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1779\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1677\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1676\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1677\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1678\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1679\u001b[39m         [\n\u001b[32m   1680\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1681\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1682\u001b[39m         ]\n\u001b[32m   1683\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1670\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1669\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1670\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1646\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1643\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1644\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1647\u001b[39m byte_encoder = bytes_to_unicode()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/tiktoken/load.py:162\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     ret = {}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/tiktoken/load.py:52\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m cache_key = hashlib.sha1(\u001b[43mblobpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m()).hexdigest()\n\u001b[32m     54\u001b[39m cache_path = os.path.join(cache_dir, cache_key)\n",
            "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# chạy 1 lần thôi vì file đã lưu trong drive\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 1. Tải pipeline phân tích cảm xúc\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mĐang tải mô hình NLP...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sentiment_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ✅ Tắt fast tokenizer để không bị lỗi\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m...Tải mô hình NLP thành công!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# --- CHỌN 1 TRONG 2 LỰA CHỌN DƯỚI ĐÂY ---\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Lựa chọn A: Chạy thử nghiệm trên 1000 dòng (để test code)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# data_to_process = df.head(1000).copy()\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Lựa chọn B: Chạy trên TOÀN BỘ dữ liệu (Dùng khi chạy thật - SẼ RẤT LÂU)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1078\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1080\u001b[39m         tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1073\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1071\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m         tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1077\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m load_tokenizer:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1159\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2097\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2094\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2095\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2097\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2099\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2100\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2343\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2341\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2342\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2343\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2345\u001b[39m     logger.info(\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2348\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[39m, in \u001b[36mXLMRobertaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     94\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[32m    106\u001b[39m     mask_token = AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/github/GMP1/server/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:1785\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1781\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1782\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1783\u001b[39m     ).converted()\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1786\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1787\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1788\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1789\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
          ]
        }
      ],
      "source": [
        "# chạy 1 lần thôi vì file đã lưu trong drive\n",
        "# 1. Tải pipeline phân tích cảm xúc\n",
        "print(\"Đang tải mô hình NLP...\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
        "    tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
        "    use_fast=False  # ✅ Tắt fast tokenizer để không bị lỗi\n",
        ")\n",
        "print(\"...Tải mô hình NLP thành công!\")\n",
        "\n",
        "# --- CHỌN 1 TRONG 2 LỰA CHỌN DƯỚI ĐÂY ---\n",
        "\n",
        "# Lựa chọn A: Chạy thử nghiệm trên 1000 dòng (để test code)\n",
        "# data_to_process = df.head(1000).copy()\n",
        "\n",
        "# Lựa chọn B: Chạy trên TOÀN BỘ dữ liệu (Dùng khi chạy thật - SẼ RẤT LÂU)\n",
        "data_to_process = df.copy()\n",
        "# ---------------------------------------------\n",
        "\n",
        "titles_list = data_to_process['title'].tolist()\n",
        "\n",
        "# 3. Chạy phân tích cảm xúc\n",
        "print(f\"Bắt đầu phân tích cảm xúc cho {len(titles_list)} tiêu đề...\")\n",
        "sentiment_results = sentiment_pipeline(titles_list, batch_size=64, truncation=True)\n",
        "print(\"...Hoàn thành phân tích cảm xúc.\")\n",
        "\n",
        "# 4. Gán kết quả trở lại DataFrame\n",
        "label_map = {'LABEL_0': 'negative', 'LABEL_1': 'neutral', 'LABEL_2': 'positive'}\n",
        "data_to_process['sentiment_raw'] = [result['label'] for result in sentiment_results]\n",
        "data_to_process['sentiment'] = data_to_process['sentiment_raw'].map(label_map)\n",
        "\n",
        "# 5. --- LƯU KẾT QUẢ RA DRIVE ---\n",
        "#    Đây là bước quan trọng để bạn không phải chạy lại Giai đoạn 1\n",
        "output_csv_name = 'data_da_gan_nhan_sentiment.csv'\n",
        "output_csv_path = os.path.join(BASE_DRIVE_PATH, output_csv_name)\n",
        "\n",
        "print(f\"\\nĐang lưu kết quả (với cột 'sentiment') ra Drive tại:\")\n",
        "print(output_csv_path)\n",
        "data_to_process.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "print(\"...Lưu thành công!\")\n",
        "\n",
        "# 6. Xem kết quả Giai đoạn 1\n",
        "print(\"\\nPhân bổ các nhãn cảm xúc:\")\n",
        "print(data_to_process['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "racod4i8sUz0"
      },
      "outputs": [],
      "source": [
        "# chạy 1 lần thôi vì file đã lưu trong drive\n",
        "# --- Cell 6 ( Gộp, Xử lý và Lưu file) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. --- ĐỊNH NGHĨA TÊN FILE ---\n",
        "TECHNICAL_FILE_NAME = 'data_price.csv'\n",
        "SENTIMENT_FILE_NAME = 'data_da_gan_nhan_sentiment.csv'\n",
        "MERGED_OUTPUT_NAME = 'data_merged_final_for_model.csv' # <-- File kết quả\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# 2. Đặt đường dẫn\n",
        "sentiment_csv_path = os.path.join(BASE_DRIVE_PATH, SENTIMENT_FILE_NAME)\n",
        "technical_csv_path = os.path.join(BASE_DRIVE_PATH, TECHNICAL_FILE_NAME)\n",
        "merged_output_path = os.path.join(BASE_DRIVE_PATH, MERGED_OUTPUT_NAME)\n",
        "\n",
        "try:\n",
        "    # 3. Đọc 2 file dữ liệu\n",
        "    print(f\"Đang đọc file sentiment (BÊN TRÁI) từ: {sentiment_csv_path}\")\n",
        "    df_sentiment = pd.read_csv(sentiment_csv_path)\n",
        "    print(\"...Đọc file sentiment thành công.\")\n",
        "\n",
        "    print(f\"Đang đọc file kỹ thuật (BÊN PHẢI) từ: {technical_csv_path}\")\n",
        "    df_price = pd.read_csv(technical_csv_path)\n",
        "    print(\"...Đọc file kỹ thuật thành công.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"LỖI: Không tìm thấy file. {e}\")\n",
        "    raise\n",
        "\n",
        "# 4. --- CHUẨN BỊ FILE SENTIMENT (TRÁI) ---\n",
        "print(\"\\nChuẩn bị file Sentiment...\")\n",
        "if 'datepost' in df_sentiment.columns:\n",
        "    print(\"  Đã tìm thấy cột 'datepost'. Sẽ xóa (drop) cột này.\")\n",
        "    df_sentiment = df_sentiment.drop(columns=['datepost'])\n",
        "if 'Date' in df_sentiment.columns and 'date' not in df_sentiment.columns:\n",
        "    df_sentiment.rename(columns={'Date': 'date'}, inplace=True)\n",
        "df_sentiment['date'] = pd.to_datetime(df_sentiment['date']).dt.normalize()\n",
        "df_sentiment['Ticker'] = df_sentiment['Ticker'].astype(str).str.strip().str.upper()\n",
        "print(\"...Chuẩn bị file Sentiment xong.\")\n",
        "\n",
        "# 5. --- CHUẨN BỊ FILE KỸ THUẬT (PHẢI) ---\n",
        "print(\"\\nChuẩn bị file Kỹ thuật...\")\n",
        "technical_indicators_cols = [\n",
        "    'ma20', 'ma50', 'ema12', 'ema26', 'macd', 'macd_hist', 'macd_signal',\n",
        "    'bb_lower', 'bb_mid', 'bb_upper', 'BBB_20_2.0', 'BBP_20_2.0',\n",
        "    'atr14', 'rsi14', 'stoch_k', 'stoch_d', 'obv', 'vol_ma20'\n",
        "]\n",
        "if 'Date' in df_price.columns and 'date' not in df_price.columns:\n",
        "    df_price.rename(columns={'Date': 'date'}, inplace=True)\n",
        "if 'Ticker' in df_price.columns and 'ticker' not in df_price.columns:\n",
        "     df_price.rename(columns={'Ticker': 'ticker'}, inplace=True)\n",
        "if 'ticker' in df_price.columns:\n",
        "    df_price.rename(columns={'ticker': 'Ticker'}, inplace=True)\n",
        "cols_to_get_from_price = ['date', 'Ticker'] + [col for col in technical_indicators_cols if col in df_price.columns]\n",
        "df_price_clean = df_price[cols_to_get_from_price]\n",
        "print(f\"  Các cột sẽ lấy từ file kỹ thuật: {cols_to_get_from_price[2:]}\")\n",
        "df_price_clean['date'] = pd.to_datetime(df_price_clean['date']).dt.normalize()\n",
        "df_price_clean['Ticker'] = df_price_clean['Ticker'].astype(str).str.strip().str.upper()\n",
        "print(\"...Chuẩn bị file Kỹ thuật xong.\")\n",
        "\n",
        "# 6. GỘP (MERGE) HAI DATAFRAME\n",
        "print(\"\\nĐang gộp (Left Join) file sentiment và file kỹ thuật...\")\n",
        "final_data_merged = pd.merge(\n",
        "    df_sentiment,\n",
        "    df_price_clean,\n",
        "    on=['date', 'Ticker'],\n",
        "    how='left'\n",
        ")\n",
        "print(\"...Gộp thành công!\")\n",
        "\n",
        "# 7. TẠO FEATURES MỚI (Sentiment Score và Target)\n",
        "print(\"\\nĐang tạo features và target...\")\n",
        "sentiment_score_map = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
        "final_data_merged['sentiment_raw_cleaned'] = final_data_merged['sentiment_raw'].astype(str).str.strip()\n",
        "final_data_merged['sentiment_score'] = final_data_merged['sentiment_raw_cleaned'].map(sentiment_score_map)\n",
        "final_data_merged['price_diff'] = final_data_merged['lead1_close'] - final_data_merged['Close']\n",
        "print(\"...Tạo features và target thành công.\")\n",
        "\n",
        "# 8. LỌC CỘT VÀ LÀM SẠCH (DROPNA)\n",
        "print(\"Đang làm sạch dữ liệu (dropna)...\")\n",
        "numeric_features = (\n",
        "    ['Volume', 'sentiment_score', 'Open', 'High', 'Low', 'Close'] +\n",
        "    [col for col in technical_indicators_cols if col in final_data_merged.columns]\n",
        ")\n",
        "target = 'price_diff'\n",
        "# Giữ lại các cột khóa để sắp xếp\n",
        "cols_to_check = ['date', 'Ticker'] + numeric_features + [target]\n",
        "final_data_cleaned = final_data_merged.dropna(subset=cols_to_check)\n",
        "print(f\"Số dòng dữ liệu còn lại SAU KHI bỏ NaN: {len(final_data_cleaned)}\")\n",
        "\n",
        "# 9. LƯU FILE ĐÃ GỘP VÀ SẠCH RA DRIVE\n",
        "print(f\"\\nĐang lưu file đã gộp và làm sạch ra Drive tại: {merged_output_path}\")\n",
        "final_data_cleaned.to_csv(merged_output_path, index=False, encoding='utf-8-sig')\n",
        "print(\"...Lưu file thành công! Bạn có thể chạy Cell 7.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tePa_CRXREa"
      },
      "outputs": [],
      "source": [
        "# chạy 1 lần thôi vì file đã lưu trong drive\n",
        "# --- Cell 7 (Lọc Cột, Xử lý Outlier) ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Tên file đầu vào (từ Cell 6)\n",
        "MERGED_INPUT_NAME = 'data_merged_final_for_model.csv'\n",
        "# 2. Tên file đầu ra (đã xử lý)\n",
        "CLEANED_OUTPUT_NAME = 'data_cleaned_no_outliers.csv'\n",
        "\n",
        "input_path = os.path.join(BASE_DRIVE_PATH, MERGED_INPUT_NAME)\n",
        "output_path = os.path.join(BASE_DRIVE_PATH, CLEANED_OUTPUT_NAME)\n",
        "\n",
        "try:\n",
        "    # 3. Đọc file\n",
        "    print(f\"Đang đọc file đã gộp từ: {input_path}\")\n",
        "    df = pd.read_csv(input_path)\n",
        "    print(f\"...Đọc thành công {len(df)} dòng.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"LỖI: Không tìm thấy file {MERGED_INPUT_NAME}.\")\n",
        "    print(\"Vui lòng chạy lại Cell 6 để tạo file này trước.\")\n",
        "    raise\n",
        "\n",
        "# 4. --- LỌC CỘT (THEO YÊU CẦU CỦA BẠN) ---\n",
        "cols_to_keep = [\n",
        "    'Ticker', 'Market', 'date', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'prev5_date', 'ma20', 'ema26', 'macd', 'macd_hist', 'macd_signal',\n",
        "    'rsi14', 'vol_ma20', 'sentiment_score', 'lead1_close'\n",
        "]\n",
        "existing_cols_to_keep = [col for col in cols_to_keep if col in df.columns]\n",
        "print(f\"\\nĐã lọc, giữ lại {len(existing_cols_to_keep)} cột.\")\n",
        "df_filtered = df[existing_cols_to_keep].copy()\n",
        "\n",
        "# 5. --- XỬ LÝ OUTLIER (Sử dụng IQR) ---\n",
        "numeric_cols_for_outlier = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume', 'ma20', 'ema26', 'macd',\n",
        "    'macd_hist', 'macd_signal', 'rsi14', 'vol_ma20', 'sentiment_score'\n",
        "]\n",
        "numeric_cols_for_outlier = [col for col in numeric_cols_for_outlier if col in df_filtered.columns]\n",
        "\n",
        "print(\"Đang xử lý Outlier (IQR)...\")\n",
        "df_no_outlier = df_filtered.copy()\n",
        "initial_rows = len(df_no_outlier)\n",
        "\n",
        "for col in numeric_cols_for_outlier:\n",
        "    Q1 = df_no_outlier[col].quantile(0.25)\n",
        "    Q3 = df_no_outlier[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    df_no_outlier = df_no_outlier[\n",
        "        (df_no_outlier[col] >= lower_bound) & (df_no_outlier[col] <= upper_bound)\n",
        "    ]\n",
        "\n",
        "print(f\"Số dòng ban đầu: {initial_rows}\")\n",
        "print(f\"Số dòng sau khi loại bỏ Outlier: {len(df_no_outlier)}\")\n",
        "df_processed = df_no_outlier.copy()\n",
        "\n",
        "# 6. --- LƯU FILE ĐÃ XỬ LÝ RA DRIVE ---\n",
        "print(f\"\\nĐang lưu file đã xử lý outlier ra Drive tại: {output_path}\")\n",
        "df_processed.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "print(\"...Lưu file thành công! Bạn có thể chạy Cell 8.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD6K-bFgazkY"
      },
      "outputs": [],
      "source": [
        "# --- Chia và Lưu Train/Test ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Tên file đã xử lý (từ Cell 7)\n",
        "CLEANED_INPUT_NAME = 'data_cleaned_no_outliers.csv'\n",
        "cleaned_path = os.path.join(BASE_DRIVE_PATH, CLEANED_INPUT_NAME)\n",
        "\n",
        "try:\n",
        "    # 2. Đọc file\n",
        "    print(f\"Đang đọc file đã xử lý outlier từ: {cleaned_path}\")\n",
        "    df_processed = pd.read_csv(cleaned_path)\n",
        "    print(f\"...Đọc thành công {len(df_processed)} dòng.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"LỖI: Không tìm thấy file {CLEANED_INPUT_NAME}.\")\n",
        "    print(\"Vui lòng chạy lại Cell 7 để tạo file này trước.\")\n",
        "    raise\n",
        "\n",
        "# 3. --- QUAY LẠI TARGET GỐC: 'lead1_close' ---\n",
        "target = 'lead1_close'\n",
        "print(f\"\\nĐã đặt target là: {target}\")\n",
        "# -----------------------------------------------\n",
        "\n",
        "# 4. --- ĐỊNH NGHĨA FEATURES (\"BẪY\") ---\n",
        "categorical_features = ['Market']\n",
        "numeric_features = [\n",
        "    'Open', 'High', 'Low', 'Close',\n",
        "    'Volume', 'ma20', 'ema26', 'macd',\n",
        "    'macd_hist', 'macd_signal', 'rsi14', 'vol_ma20', 'sentiment_score'\n",
        "]\n",
        "# Lọc lại\n",
        "numeric_features = [col for col in numeric_features if col in df_processed.columns]\n",
        "categorical_features = [col for col in categorical_features if col in df_processed.columns]\n",
        "features_to_use = numeric_features + categorical_features\n",
        "\n",
        "print(f\"Features sẽ dùng ({len(features_to_use)}): {features_to_use}\")\n",
        "print(f\"Target sẽ dùng: {target}\")\n",
        "\n",
        "# 5. --- CHIA TRAIN/TEST (THEO THỜI GIAN) ---\n",
        "df_processed['date'] = pd.to_datetime(df_processed['date'])\n",
        "df_processed = df_processed.sort_values(by='date').reset_index(drop=True)\n",
        "\n",
        "# Lấy X và y MỘT LẦN NỮA sau khi sắp xếp\n",
        "X = df_processed[features_to_use]\n",
        "y = df_processed[target] # <-- y bây giờ là 'lead1_close'\n",
        "\n",
        "# Tỉ lệ 70/30\n",
        "test_percent = 0.3\n",
        "test_point = int(len(X) * (1 - test_percent))\n",
        "\n",
        "if test_point == 0 or len(X) == 0:\n",
        "    print(\"LỖI: Không đủ dữ liệu để chia train/test.\")\n",
        "else:\n",
        "    X_train = X.iloc[:test_point] # 70% dữ liệu đầu (cũ nhất)\n",
        "    X_test = X.iloc[test_point:]  # 30% dữ liệu cuối (mới nhất)\n",
        "    y_train = y.iloc[:test_point]\n",
        "    y_test = y.iloc[test_point:]\n",
        "\n",
        "    print(f\"\\nKích thước tập Train (70%): {len(X_train)} dòng\")\n",
        "    print(f\"Kích thước tập Test (30%): {len(X_test)} dòng\")\n",
        "\n",
        "    # 6. --- (PHẦN MỚI) LƯU TẬP TRAIN VÀ TEST RA FILE ---\n",
        "    print(\"\\nĐang lưu 2 tập Train/Test ra file (để bạn chạy test lại)...\")\n",
        "\n",
        "    # 6.1. Gộp X_train và y_train\n",
        "    # (Chúng ta cần cả các cột 'date' và 'Ticker' để tham chiếu)\n",
        "    train_indices = X_train.index\n",
        "    train_df_to_save = df_processed.loc[train_indices].copy()\n",
        "\n",
        "    # 6.2. Gộp X_test và y_test\n",
        "    test_indices = X_test.index\n",
        "    test_df_to_save = df_processed.loc[test_indices].copy()\n",
        "\n",
        "    # 6.3. Định nghĩa đường dẫn\n",
        "    train_output_path = os.path.join(BASE_DRIVE_PATH, 'train_data.csv')\n",
        "    test_output_path = os.path.join(BASE_DRIVE_PATH, 'test_data.csv')\n",
        "\n",
        "    # 6.4. Lưu file\n",
        "    train_df_to_save.to_csv(train_output_path, index=False, encoding='utf-8-sig')\n",
        "    test_df_to_save.to_csv(test_output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(f\"...Đã lưu file Train tại: {train_output_path}\")\n",
        "    print(f\"...Đã lưu file Test tại: {test_output_path}\")\n",
        "    # --- (KẾT THÚC PHẦN MỚI) ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BRQKTK8a6Ju"
      },
      "outputs": [],
      "source": [
        "# --- So sánh mô hình, thêm Ridge/Lasso ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# --- Thư viện các mô hình để so sánh ---\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# 1. Tạo 'preprocessor' (Scaling + OneHotEncoding)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "print(\"Đã tạo Preprocessor (Scaling + OneHotEncoding).\")\n",
        "\n",
        "# 2. Định nghĩa các mô hình\n",
        "#    (Thêm Ridge và Lasso theo yêu cầu của bạn)\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge (Chống Overfit)': Ridge(alpha=1.0, random_state=42),\n",
        "    'Lasso (Chống Overfit)': Lasso(alpha=0.1, random_state=42, max_iter=2000),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# 3. Huấn luyện và đánh giá hàng loạt\n",
        "print(f\"Bắt đầu huấn luyện và so sánh {len(models)} mô hình (trên dữ liệu SẠCH)...\")\n",
        "results_list = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nĐang huấn luyện: {name}...\")\n",
        "\n",
        "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                         ('model', model)])\n",
        "\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        'Model': name,\n",
        "        'Test R2': r2_score(y_test, y_pred_test),\n",
        "        'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
        "        'Train R2': r2_score(y_train, y_pred_train),\n",
        "        'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
        "        'Trained Pipeline': clf\n",
        "    }\n",
        "    results_list.append(metrics)\n",
        "    print(f\"...Hoàn thành {name} trong {train_time:.2f} giây.\")\n",
        "\n",
        "# 4. --- TẠO BẢNG TỔNG HỢP KẾT QUẢ ---\n",
        "print(\"\\n--- BẢNG TỔNG HỢP KẾT QUẢ (Mô hình SẠCH) ---\")\n",
        "results_df = pd.DataFrame(results_list)\n",
        "results_df = results_df.sort_values(by='Test MAE', ascending=True)\n",
        "\n",
        "print(results_df.drop(columns=['Trained Pipeline']).to_markdown(index=False, floatfmt=\".4f\"))\n",
        "\n",
        "# 5. LƯU BẢNG KẾT QUẢ\n",
        "results_table_path = os.path.join(BASE_DRIVE_PATH, 'ket_qua_so_sanh_models_SACH.csv')\n",
        "print(f\"\\nĐang lưu bảng kết quả ra file: {results_table_path}\")\n",
        "results_df.drop(columns=['Trained Pipeline']).to_csv(results_table_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "# 6. Chọn ra mô hình tốt nhất để chẩn đoán\n",
        "best_model_pipeline = results_df.iloc[0]['Trained Pipeline']\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"\\nĐã chọn mô hình tốt nhất ({best_model_name})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3I6ee7Fz6Iw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mBjCEQqICMj"
      },
      "outputs": [],
      "source": [
        "# ---  Định nghĩa các công cụ Chẩn đoán ---\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import TimeSeriesSplit, learning_curve, cross_validate\n",
        "import statsmodels.api as sm\n",
        "\n",
        "print(\"Đã import các thư viện chẩn đoán: TimeSeriesSplit, learning_curve, cross_validate\")\n",
        "print(\"Định nghĩa các hàm: run_time_series_cv, plot_learning_curve, plot_residuals\")\n",
        "\n",
        "# 1. CÔNG CỤ: K-FOLD CROSS-VALIDATION (dành cho Time Series)\n",
        "def run_time_series_cv(pipeline, X, y, n_splits=5):\n",
        "    \"\"\"Chạy Cross-Validation dùng TimeSeriesSplit và in kết quả.\"\"\"\n",
        "    print(f\"\\n--- 1. Chẩn đoán: Time Series Cross-Validation ({n_splits}-Fold) ---\")\n",
        "\n",
        "    cv_splitter = TimeSeriesSplit(n_splits=n_splits)\n",
        "    scoring_metrics = ['r2', 'neg_mean_absolute_error']\n",
        "\n",
        "    print(\"Đang chạy CV...\")\n",
        "\n",
        "    # (Đã đổi 'return_train_scores' thành 'return_train_score')\n",
        "    scores = cross_validate(pipeline, X, y, cv=cv_splitter,\n",
        "                            scoring=scoring_metrics, n_jobs=-1,\n",
        "                            return_train_score=True) # <-- SỬA LỖI Ở ĐÂY\n",
        "\n",
        "    print(\"...CV hoàn thành.\")\n",
        "\n",
        "    # Xử lý kết quả\n",
        "    mean_test_r2 = np.mean(scores['test_r2'])\n",
        "    mean_test_mae = -np.mean(scores['test_neg_mean_absolute_error'])\n",
        "    mean_train_r2 = np.mean(scores['train_r2'])\n",
        "    mean_train_mae = -np.mean(scores['train_neg_mean_absolute_error'])\n",
        "\n",
        "    print(f\"  Kết quả CV trung bình trên {n_splits} fold:\")\n",
        "    print(f\"  - Test R2 (Trung bình): {mean_test_r2:.4f}\")\n",
        "    print(f\"  - Test MAE (Trung bình): {mean_test_mae:.4f}\")\n",
        "    print(f\"  - Train R2 (Trung bình): {mean_train_r2:.4f}\")\n",
        "    print(f\"  - Train MAE (Trung...\"\")\")\n",
        "\n",
        "    if mean_train_mae < (mean_test_mae * 0.7): # Nếu train tốt hơn test 30%\n",
        "        print(\"  => KẾT LUẬN: CÓ DẤU HIỆU OVERFITTING (Train tốt hơn Test đáng kể).\")\n",
        "    else:\n",
        "        print(\"  => KẾT LUẬN: Overfitting ở mức chấp nhận được.\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "# 2. CÔNG CỤ: LEARNING CURVE\n",
        "def plot_learning_curve(pipeline, X, y, n_splits=5):\n",
        "    \"\"\"Vẽ Learning Curve để chẩn đoán Overfitting/Underfitting.\"\"\"\n",
        "    print(\"\\n--- 2. Chẩn đoán: Vẽ Learning Curve ---\")\n",
        "\n",
        "    cv_splitter = TimeSeriesSplit(n_splits=n_splits)\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        pipeline, X, y, cv=cv_splitter,\n",
        "        scoring='neg_mean_absolute_error', # Dùng MAE\n",
        "        n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.2, 1.0, 5) # 5 bước (20% -> 100% data)\n",
        "    )\n",
        "\n",
        "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
        "    test_scores_mean = -np.mean(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.grid()\n",
        "    plt.title(\"Learning Curve (Đường cong học tập)\")\n",
        "    plt.xlabel(\"Kích thước tập Huấn luyện (Training examples)\")\n",
        "    plt.ylabel(\"Lỗi MAE\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Lỗi Training\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Lỗi Validation (CV)\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.ylim(bottom=0)\n",
        "    plt.show()\n",
        "\n",
        "    if train_scores_mean[-1] < (test_scores_mean[-1] * 0.7):\n",
        "        print(\"  => KẾT LUẬN: Khoảng cách giữa 2 đường lớn -> Dấu hiệu Overfitting (High Variance).\")\n",
        "    elif test_scores_mean[-1] > (train_scores_mean[-1] * 1.5):\n",
        "         print(\"  => KẾT LUẬN: Cả 2 đường đều hội tụ ở mức lỗi cao -> Dấu hiệu Underfitting (High Bias).\")\n",
        "    else:\n",
        "         print(\"  => KẾT LUẬN: Hai đường hội tụ tốt.\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "\n",
        "# 3. CÔNG CỤ: RESIDUAL PLOT\n",
        "def plot_residuals(y_true, y_pred, model_name):\n",
        "    \"\"\"Vẽ biểu đồ Residual (Phần dư) để chẩn đoán Bias.\"\"\"\n",
        "    print(\"\\n--- 3. Chẩn đoán: Vẽ Residual Plot ---\")\n",
        "\n",
        "    residuals = y_true - y_pred\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.scatterplot(x=y_pred, y=residuals, alpha=0.5)\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel(\"Giá trị Dự đoán (Predicted Values)\")\n",
        "    plt.ylabel(\"Phần dư (Residuals = True - Predicted)\")\n",
        "    plt.title(f\"Residual Plot - Mô hình {model_name}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    mean_residual = np.mean(residuals)\n",
        "    print(f\"  Residuals (phần dư) trung bình: {mean_residual:.4f}\")\n",
        "    if abs(mean_residual) > 0.1:\n",
        "         print(\"  => KẾT LUẬN: Residuals không tập trung quanh 0 -> Dấu hiệu mô hình bị 'Thiên kiến' (Bias).\")\n",
        "    else:\n",
        "         print(\"  => KẾT LUẬN: Residuals phân bổ tốt quanh 0.\")\n",
        "    print(\"--------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D45ucPF5pjW"
      },
      "outputs": [],
      "source": [
        "# ---  Chạy chẩn đoán ---\n",
        "print(f\"=== BẮT ĐẦU CHẨN ĐOÁN TOÀN DIỆN MÔ HÌNH: {best_model_name} ===\")\n",
        "# 1. Chạy Time Series K-Fold CV\n",
        "\n",
        "#    (Chạy trên toàn bộ dữ liệu X, y để có đánh giá CV tin cậy nhất)\n",
        "run_time_series_cv(best_model_pipeline, X, y, n_splits=5)\n",
        "# 2. Chạy Learning Curve\n",
        "plot_learning_curve(best_model_pipeline, X, y, n_splits=5)\n",
        "\n",
        "# 3. Chạy Residual Plot\n",
        "\n",
        "#    (Chạy trên tập Test để xem hiệu suất trên dữ liệu mới)\n",
        "\n",
        "print(\"\\n...Đang tạo dự đoán trên tập Test để vẽ Residual Plot...\")\n",
        "y_pred_test_for_residuals = best_model_pipeline.predict(X_test)\n",
        "plot_residuals(y_test, y_pred_test_for_residuals, best_model_name)\n",
        "print(\"\\n=== HOÀN THÀNH CHẨN ĐOÁN ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGm4SV5mIEnI"
      },
      "outputs": [],
      "source": [
        "# ---  Biểu đồ kết quả cuối cùng ---\n",
        "\n",
        "print(f\"Đang vẽ biểu đồ Scatter cuối cùng cho mô hình: {best_model_name}...\")\n",
        "\n",
        "# Lấy lại dự đoán từ Cell 11 (hoặc dự đoán lại)\n",
        "\n",
        "y_pred = best_model_pipeline.predict(X_test)\n",
        "\n",
        "plot_df = pd.DataFrame({'Chênh lệch Thực tế (y_test)': y_test,\n",
        "\n",
        "                          'Chênh lệch Dự đoán (y_pred)': y_pred})\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "sns.scatterplot(data=plot_df,\n",
        "\n",
        "                x='Chênh lệch Thực tế (y_test)',\n",
        "\n",
        "                y='Chênh lệch Dự đoán (y_pred)',\n",
        "\n",
        "                alpha=0.5,\n",
        "\n",
        "                label=f\"Dự đoán của ({best_model_name})\")\n",
        "\n",
        "min_val = min(y_test.min(), y_pred.min())\n",
        "\n",
        "max_val = max(y_test.max(), y_pred.max())\n",
        "\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Dự đoán Hoàn hảo (y = x)')\n",
        "\n",
        "plt.title(f'Biểu đồ Scatter (SAU KHI FIX LEAKAGE): Mô hình {best_model_name}')\n",
        "\n",
        "plt.xlabel('Chênh lệch Thực tế (y_test)')\n",
        "\n",
        "plt.ylabel('Chênh lệch Dự đoán (y_pred)')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "\n",
        "plt.axis('equal')\n",
        "\n",
        "plot_path = os.path.join(BASE_DRIVE_PATH, f'bieu_do_best_model_SACH.png')\n",
        "\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"...Đã lưu biểu đồ cuối cùng tại: {plot_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_qlRPw50TE1"
      },
      "outputs": [],
      "source": [
        "# --- \"Đóng gói\" Model đã huấn luyện---\n",
        "import joblib\n",
        "print(f\"--- Bắt đầu đóng gói (lưu) mô hình tốt nhất ---\")\n",
        "try:\n",
        "    # 1. Lấy mô hình TỐT NHẤT (đã được huấn luyện trên 70% X_train)\n",
        "    #    (Biến 'best_model_pipeline' được tạo ở Cell 9)\n",
        "    model_to_save = best_model_pipeline\n",
        "    model_name_to_save = best_model_name\n",
        "\n",
        "    print(f\"Mô hình được chọn để lưu: {model_name_to_save}\")\n",
        "    print(\"Mô hình này đã được huấn luyện trên 70% dữ liệu (Tập Train).\")\n",
        "\n",
        "except NameError as e:\n",
        "    print(\"LỖI: Không tìm thấy 'best_model_pipeline'.\")\n",
        "    print(\"Vui lòng chạy lại Cell 9 trước khi chạy ô này.\")\n",
        "    raise\n",
        "\n",
        "# 2. KHÔNG HUẤN LUYỆN LẠI (Theo yêu cầu của bạn)\n",
        "#    (Chúng ta bỏ qua bước .fit(X_full, y_full))\n",
        "\n",
        "# 3. LƯU MODEL RA FILE\n",
        "model_output_path = os.path.join(BASE_DRIVE_PATH, 'stock_predictor_model.joblib')\n",
        "print(f\"\\nĐang lưu mô hình đã huấn luyện ra file: {model_output_path}\")\n",
        "joblib.dump(model_to_save, model_output_path)\n",
        "print(\"...LƯU MODEL THÀNH CÔNG!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
